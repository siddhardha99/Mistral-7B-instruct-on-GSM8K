# ğŸ§  Fine-Tuning Mistral 7B on GSM8K with LoRA 

This project fine-tunes the [Mistral 7B Instruct](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1) large language model on the Grade School Math 8K (GSM8K) dataset using the Low-Rank Adaptation (LoRA) technique. The pipeline is optimized for Kaggle's GPU environment and leverages `bitsandbytes` for efficient 4-bit quantization.

---

## ğŸ“š Dataset

**GSM8K**: A high-quality dataset of 8,500+ grade school math word problems created by OpenAI.

- ğŸ“‚ Train: `main_train.csv`
- ğŸ“‚ Test: `main_test.csv`
- ğŸ§  Source: [`thedevastator/grade-school-math-8k-q-a`](https://www.kaggle.com/datasets/thedevastator/grade-school-math-8k-q-a)

---

## ğŸ”§ Model and Setup

- ğŸ§  Model: `Mistral-7B-Instruct-v0.1` (via Hugging Face)
- âš™ï¸ Quantization: 4-bit (NF4) using `bitsandbytes`
- ğŸ§© Fine-Tuning: LoRA via HuggingFace PEFT
- ğŸ›  Environment: Kaggle Notebook + CUDA GPU
- ğŸ“¦ Libraries: `transformers`, `datasets`, `peft`, `trl`, `bitsandbytes`

---

## ğŸ—ï¸ Prompt Format

Training prompt format:
```text
<QUESTION> [SEP] <ANSWER> <eos>
